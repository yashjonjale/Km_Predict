{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import ExPASy, SwissProt # For sequence retrieval\n",
    "import torch\n",
    "import esm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "# from Bio import pairwise2\n",
    "# from Bio.pairwise2 import format_alignment\n",
    "import time\n",
    "from Bio import Align\n",
    "emb_dim = 1280\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_sequence(uniprot_id):\n",
    "    handle = ExPASy.get_sprot_raw(uniprot_id)\n",
    "    record = SwissProt.read(handle)\n",
    "    return record.sequence\n",
    "def safe_extract_sequence(uniprot_id):\n",
    "    try:\n",
    "        return extract_sequence(uniprot_id)\n",
    "    except ValueError:\n",
    "        return None  # or '', or any default value you want to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(data, model, device, batch_converter):\n",
    "    # Prepare the input\n",
    "    # batch_converter = model.alphabet.get_batch_converter()\n",
    "    # data = lst\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    # Move the input data to the GPU if available\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "    # Encode the sequences\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens.to(device), repr_layers=[33], return_contacts=False)\n",
    "\n",
    "    # Extract the embeddings\n",
    "    token_representations = results[\"representations\"][33].cpu().numpy()\n",
    "    # print(token_embeddings.shape)  # This should give you (num_sequences, sequence_length, embedding_dim)\n",
    "    # return token_embeddings\n",
    "    protein_representations = np.mean(token_representations[:, 1:-1, :], axis=1)\n",
    "    return protein_representations ## num_sequences, embedding_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score_vecs(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "\n",
    "\n",
    "def similarity_seqs(seq1, seq2):\n",
    "    aligner = Align.PairwiseAligner()\n",
    "    aligner.mode = 'global'\n",
    "    scores = [alignment.score for alignment in aligner.align(seq1, seq2)]\n",
    "    return max(scores) / max(len(seq1), len(seq2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "## get current working directory\n",
    "cwd = os.getcwd()\n",
    "## convert to string\n",
    "cwd = str(cwd)\n",
    "# logger.info(f\"Current working directory: {cwd}\")\n",
    "# logger.info(\"Loading the ESM model\")\n",
    "model, alphabet = esm.pretrained.load_model_and_alphabet_local(model_location = \"/home/not81yan/km_predict_proj/Dataset/model/esm1b_t33_650M_UR50S.pt\")\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)  # Move the model to the GPU if available\n",
    "model.eval()\n",
    "print(\"Done\")\n",
    "\n",
    "# logger.info(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 3773\n"
     ]
    }
   ],
   "source": [
    "directory = \"/home/not81yan/km_predict_proj/Dataset\"+'/cache_dir/'\n",
    "naming_format = '*_sub.csv'  # This will match any file that ends with .json\n",
    "\n",
    "uni2seq = {}\n",
    "files = glob.glob(f'{directory}/{naming_format}')\n",
    "print(f\"Total files: {len(files)}\")\n",
    "# logger.info(f\"Total files: {len(files)}\")\n",
    "start = time.time()\n",
    "\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot = \"A5F5X1\"\n",
    "change = \"G140A\"\n",
    "\n",
    "seq = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go__________________\n",
      "\n",
      "Go2_____\n",
      "____\n"
     ]
    }
   ],
   "source": [
    "for file in files[:5]:\n",
    "    input(\"Go__________________\\n\")\n",
    "    ec_num = file.strip().split('-')[0].split('/')[-1]\n",
    "    sub_id = file.strip().split('_')[-2]\n",
    "    # print(f\"File: {file}\")\n",
    "    # print(f\"EC: {ec_num}, Sub_id: {sub_id}\")\n",
    "    # input(\"Press Enter to continue...\")\n",
    "    ## open as dataframe\n",
    "    df = pd.read_csv(file)\n",
    "    ## now add more columns to the dataframe for the esm vectors with default value '-'\n",
    "    df['seq_str'] = '-'\n",
    "    df['esm'] = '-'\n",
    "    # df['esm_wild'] = '-'\n",
    "    df['sim_seqs'] = '-'\n",
    "    # df['sim_vecs'] = '-'\n",
    "    len_cor = True\n",
    "    #iterate over the rows\n",
    "    for index, row in df.iterrows():\n",
    "        input(\"Go2_____\")\n",
    "        \n",
    "        uniprot_id = row['uniprot']\n",
    "        # print(f\"Processing {uniprot_id} and change {row['change']}\")\n",
    "        if uniprot_id not in uni2seq:\n",
    "            print(\"____\")\n",
    "            seq = safe_extract_sequence(uniprot_id)\n",
    "            print(\"____\")\n",
    "            uni2seq[uniprot_id] = seq\n",
    "        seq = uni2seq[uniprot_id]\n",
    "        if seq is None:\n",
    "            df.at[index, 'sim_seqs'] = '-'\n",
    "            # df.at[index, 'sim_vecs'] = '-'\n",
    "            df.at[index, 'esm'] = '-'\n",
    "            # df.at[index, 'esm_wild'] = '-'\n",
    "            df.drop(index, inplace=True)\n",
    "            continue\n",
    "            # len_cor = False\n",
    "            # break\n",
    "        if len(seq) > 1000:\n",
    "            len_cor = False\n",
    "            break\n",
    "        \n",
    "        \n",
    "        if row['change'] != '-':\n",
    "            seq_final = modify_seq(seq, row['change'])\n",
    "            if seq_final == \"ignore_entry\":\n",
    "                ## delete row\n",
    "                df.drop(index, inplace=True)\n",
    "                continue\n",
    "            df.at[index, 'seq_str'] = seq_final\n",
    "            data = [\n",
    "                (\"label2\", seq_final)\n",
    "            ]\n",
    "            vecs = extract_embeddings(data, model, device, batch_converter)\n",
    "            df.at[index, 'esm'] = list(vecs[0])\n",
    "            seq_sim = similarity_seqs(seq, seq_final)\n",
    "            df.at[index, 'sim_seqs'] = seq_sim\n",
    "        else:\n",
    "            # seq_final = \"ignore_entry\"\n",
    "            df.at[index, 'seq_str'] = seq\n",
    "            data = [\n",
    "                (\"label1\", seq)\n",
    "            ]\n",
    "            vecs = extract_embeddings(data, model, device, batch_converter)\n",
    "            df.at[index, 'esm'] = list(vecs[0])\n",
    "    # seq_sim tells you if mutated or not\n",
    "    # delete the mutation column\n",
    "    df = df.drop(columns=['mutation'])\n",
    "    ## save the dataframe   \n",
    "    # print(f\"Saving {ec_num}_{sub_id}_sub_esm.csv\")\n",
    "    if len_cor:\n",
    "        df.to_csv(directory+\"../refined_data/\"+f\"{ec_num}_{sub_id}_sub_esm.csv\")\n",
    "    i+=1\n",
    "    \n",
    "#         logger.info(f\"Done {i} files, time elapsed: {time.time()-start}\")\n",
    "    print(f\"Done {i} files, time elapsed: \", time.time()-start)            \n",
    "        \n",
    "print(\"Done, time elapsed: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_seqs(seq1, seq2):\n",
    "    aligner = Align.PairwiseAligner()\n",
    "    aligner.mode = 'global'\n",
    "    scores = [alignment.score for alignment in aligner.align(seq1, seq2)]\n",
    "    return max(scores) / max(len(seq1), len(seq2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deq1='AAAAA'\n",
    "seq2='AAABA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_seqs(deq1, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
