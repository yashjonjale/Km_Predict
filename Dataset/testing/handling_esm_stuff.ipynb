{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load esm1b model\n",
    "\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_embeddings(data, model, device, batch_converter):\n",
    "#     # Prepare the input\n",
    "#     # batch_converter = model.alphabet.get_batch_converter()\n",
    "#     # data = lst\n",
    "# data = [\n",
    "#     \"MKTFFVQHLLGSALASTANPLSLRLCNLRAPSRQFQVAIMFSVFYLLLYLGTLLLFLFRRLFGFSL\",\n",
    "#     \"MKVFFVQHLLGSALASTANPLSLRLCNLRAPSRQFQVAIMFSVFYLLLYLGTLLLFLFRRLFGFSL\",\n",
    "#     \"MKVFFVQHLLGSALASTANPLSLRLCNLRAPSRQFQVAIMFSVFYLLLYLGTLLLFLFRRLFGFSL\"\n",
    "# ]\n",
    "data = [\n",
    "    (\"label1\", \"MKTFFVQHLLGSALASTANPLSLRLCNLRAPSRQFQVAIMFSVFYLLLYLGTLLLFLFRRLFGFSL\")\n",
    "]\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "# batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "# Move the input data to the GPU if available\n",
    "batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "# Encode the sequences\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens.to(device), repr_layers=[33], return_contacts=False)\n",
    "\n",
    "# Extract the embeddings\n",
    "token_representations = results[\"representations\"][33].cpu().numpy()\n",
    "import numpy as np\n",
    "# print(token_embeddings.shape)  # This should give you (num_sequences, sequence_length, embedding_dim)\n",
    "# return token_embeddings\n",
    "protein_representations = np.mean(token_representations[0, 1:-1, :], axis=0)\n",
    "\n",
    "\n",
    "# embeddings = extract_embeddings(sequences, model, device,batch_converter)\n",
    "\n",
    "# print(token_embeddings.shape)  # This should give you (num_sequences, sequence_length, embedding_dim)\n",
    "\n",
    "## Extracting the esm embeddings in batches \n",
    "\n",
    "def extract_embeddings(data, model, device, batch_converter):\n",
    "    # Prepare the input\n",
    "    # batch_converter = model.alphabet.get_batch_converter()\n",
    "    # data = lst\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    # Move the input data to the GPU if available\n",
    "    batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "    # Encode the sequences\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens.to(device), repr_layers=[33], return_contacts=False)\n",
    "\n",
    "    # Extract the embeddings\n",
    "    token_representations = results[\"representations\"][33].cpu().numpy()\n",
    "    import numpy as np\n",
    "    # print(token_embeddings.shape)  # This should give you (num_sequences, sequence_length, embedding_dim)\n",
    "    # return token_embeddings\n",
    "    protein_representations = np.mean(token_representations[:, 1:-1, :], axis=1)\n",
    "    return protein_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logits', 'representations'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = results['representations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
