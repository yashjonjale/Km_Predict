{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from multiprocessing import Pool\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansPyTorch:\n",
    "    def __init__(self, num_clusters, num_iterations=100, batch_size=1000, device='cuda'):\n",
    "        self.num_clusters = num_clusters\n",
    "        self.num_iterations = num_iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.cluster_centers = None\n",
    "        self.device = device\n",
    "\n",
    "    def fit(self, data):\n",
    "        num_samples, num_features = data.shape\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "        initial_indices = np.random.choice(num_samples, self.num_clusters, replace=False)\n",
    "        self.cluster_centers = data_tensor[initial_indices].clone()\n",
    "\n",
    "        def compute_distance(x, centers):\n",
    "            return torch.cdist(x, centers, p=2)  # Euclidean distance\n",
    "\n",
    "        for iteration in range(self.num_iterations):\n",
    "            print(f\"Iteration {iteration + 1}/{self.num_iterations}\")\n",
    "            all_assignments = []\n",
    "            for batch in DataLoader(TensorDataset(data_tensor), batch_size=self.batch_size, shuffle=False):\n",
    "                distances = compute_distance(batch[0], self.cluster_centers)\n",
    "                assignments = torch.argmin(distances, dim=1)\n",
    "                all_assignments.append(assignments)\n",
    "            all_assignments = torch.cat(all_assignments)\n",
    "            new_centers = []\n",
    "            for cluster_idx in range(self.num_clusters):\n",
    "                cluster_points = data_tensor[all_assignments == cluster_idx]\n",
    "                if len(cluster_points) > 0:\n",
    "                    new_center = cluster_points.mean(dim=0)\n",
    "                else:\n",
    "                    new_center = self.cluster_centers[cluster_idx]  # Keep the same if no points are assigned\n",
    "                new_centers.append(new_center)\n",
    "            self.cluster_centers = torch.stack(new_centers)\n",
    "\n",
    "    def predict(self, data):\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "        def compute_distance(x, centers):\n",
    "            return torch.cdist(x, centers, p=2)  # Euclidean distance\n",
    "        final_assignments = []\n",
    "        for batch in DataLoader(TensorDataset(data_tensor), batch_size=self.batch_size, shuffle=False):\n",
    "            distances = compute_distance(batch[0], self.cluster_centers)\n",
    "            assignments = torch.argmin(distances, dim=1)\n",
    "            final_assignments.append(assignments)\n",
    "        final_assignments = torch.cat(final_assignments)\n",
    "        return final_assignments.cpu().numpy()\n",
    "\n",
    "    def get_cluster_centers(self):\n",
    "        return self.cluster_centers.cpu()\n",
    "\n",
    "    def compute_inertia(self, data):\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32).to(self.device)\n",
    "        def compute_distance(x, centers):\n",
    "            return torch.cdist(x, centers, p=2)  # Euclidean distance\n",
    "        inertia = 0.0\n",
    "        for batch in DataLoader(TensorDataset(data_tensor), batch_size=self.batch_size, shuffle=False):\n",
    "            distances = compute_distance(batch[0], self.cluster_centers)\n",
    "            min_distances = torch.min(distances, dim=1).values\n",
    "            inertia += torch.sum(min_distances**2).item()\n",
    "        return inertia\n",
    "\n",
    "    def visualize_clusters(self, data, assignments):\n",
    "        pca = PCA(n_components=2)\n",
    "        data_reduced = pca.fit_transform(data)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(self.num_clusters):\n",
    "            cluster_points = data_reduced[assignments == i]\n",
    "            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}')\n",
    "        plt.legend()\n",
    "        plt.title('Cluster Assignments Visualization')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_cluster_graph(self, data, assignments, labels, output_file=\"cluster_graph.html\"):\n",
    "        G = nx.Graph()\n",
    "        cluster_centers = self.get_cluster_centers().numpy()\n",
    "        assignments = np.array(assignments)\n",
    "        labels = np.array(labels)\n",
    "        cluster_sizes = [(assignments == i).sum().item() for i in range(self.num_clusters)]\n",
    "        cluster_labels = []\n",
    "        for i in range(self.num_clusters):\n",
    "            cluster_label_counts = np.bincount(labels[assignments == i])\n",
    "            most_common_label = cluster_label_counts.argmax()\n",
    "            most_common_label_count = cluster_label_counts[most_common_label]\n",
    "            percentage = (most_common_label_count / cluster_sizes[i]) * 100\n",
    "            cluster_labels.append((most_common_label, percentage))\n",
    "        for i, (center, (label, percentage)) in enumerate(zip(cluster_centers, cluster_labels)):\n",
    "            G.add_node(i, size=cluster_sizes[i], title=f'Size: {cluster_sizes[i]}\\nLabel: {label} ({percentage:.2f}%)')\n",
    "        for i in range(self.num_clusters):\n",
    "            for j in range(i + 1, self.num_clusters):\n",
    "                distance = np.linalg.norm(cluster_centers[i] - cluster_centers[j])\n",
    "                G.add_edge(i, j, weight=float(distance))\n",
    "\n",
    "        pos = nx.spring_layout(G)\n",
    "\n",
    "        # Create edge trace\n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for edge in G.edges():\n",
    "            x0, y0 = pos[edge[0]]\n",
    "            x1, y1 = pos[edge[1]]\n",
    "            edge_x.extend([x0, x1, None])\n",
    "            edge_y.extend([y0, y1, None])\n",
    "\n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line=dict(width=0.5, color='#888'),\n",
    "            hoverinfo='none',\n",
    "            mode='lines')\n",
    "\n",
    "        # Create node trace\n",
    "        node_x = []\n",
    "        node_y = []\n",
    "        for node in G.nodes():\n",
    "            x, y = pos[node]\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "\n",
    "        node_trace = go.Scatter(\n",
    "            x=node_x, y=node_y,\n",
    "            mode='markers',\n",
    "            hoverinfo='text',\n",
    "            marker=dict(\n",
    "                showscale=True,\n",
    "                colorscale='YlGnBu',\n",
    "                size=10,\n",
    "                colorbar=dict(\n",
    "                    thickness=15,\n",
    "                    title='Node Connections',\n",
    "                    xanchor='left',\n",
    "                    titleside='right'\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        node_adjacencies = []\n",
    "        node_text = []\n",
    "        for node, adjacencies in enumerate(G.adjacency()):\n",
    "            node_adjacencies.append(len(adjacencies[1]))\n",
    "            node_info = G.nodes[node]['title']\n",
    "            node_text.append(node_info)\n",
    "\n",
    "        node_trace.marker.color = node_adjacencies\n",
    "        node_trace.text = node_text\n",
    "\n",
    "        fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                     layout=go.Layout(\n",
    "                        title='<br>Network graph visualization',\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20,l=5,r=5,t=40),\n",
    "                        annotations=[dict(\n",
    "                            text=\"\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\"\n",
    "                        )],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False))\n",
    "                        )\n",
    "\n",
    "        fig.write_html(output_file)\n",
    "        print(f\"Graph saved to {output_file}\")\n",
    "\n",
    "def kmeans_inertia(num_clusters, data, num_iterations, batch_size, device):\n",
    "    kmeans = KMeansPyTorch(num_clusters=num_clusters, num_iterations=num_iterations, batch_size=batch_size, device=device)\n",
    "    kmeans.fit(data)\n",
    "    inertia = kmeans.compute_inertia(data)\n",
    "    return inertia\n",
    "\n",
    "def plot_elbow_curve(data, max_clusters, num_iterations=100, batch_size=1000, num_processes=4, device='cuda'):\n",
    "    with Pool(num_processes) as pool:\n",
    "        print(\"grt\")\n",
    "        tasks = [(k, data, num_iterations, batch_size, device) for k in range(1, max_clusters + 1)]\n",
    "        inertias = pool.starmap(kmeans_inertia, tasks)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, max_clusters + 1), inertias, marker='o')\n",
    "    plt.title('Elbow Curve')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path =  os.getcwd()+\"/../brenda_analyse/total_esm.csv\"\n",
    "import pandas as pd\n",
    "import ast\n",
    "df = pd.read_csv(path)\n",
    "ec = df['EC_ID'].tolist()\n",
    "y = np.array(df['num_value_gm'].tolist())\n",
    "df['esm'] = df['esm'].apply(ast.literal_eval)\n",
    "X = np.array(df['esm'].tolist())\n",
    "labels1 = [int(x.split('.')[0]) for x in ec]\n",
    "labels = np.array(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.copy()\n",
    "optimal_clusters = 5  # Reduced number of clusters\n",
    "kmeans = KMeansPyTorch(num_clusters=optimal_clusters, num_iterations=100, batch_size=1000, device='cuda')\n",
    "kmeans.fit(data)\n",
    "\n",
    "assignments = kmeans.predict(data)\n",
    "\n",
    "kmeans.visualize_cluster_graph(data, assignments, labels, output_file=\"cluster_graph.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_elbow_curve(data,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
